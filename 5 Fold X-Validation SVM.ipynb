{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the 5 fold X-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 48000, 3072)\n",
      "(5, 48000, 1)\n",
      "(5, 12000, 3072)\n",
      "(5, 12000, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_x_valid_data(train_x, train_y, test_x, test_y, folds=5):\n",
    "    # flatten\n",
    "    flat_train_x = train_x.reshape((len(train_x), 32*32*3))\n",
    "    flat_train_y = train_y.reshape((len(train_y), 1))\n",
    "    flat_test_x = test_x.reshape((len(test_x), 32*32*3))\n",
    "    flat_test_y = test_y.reshape((len(test_y), 1))\n",
    "    \n",
    "    # combine\n",
    "    all_x = np.vstack((flat_train_x, flat_test_x))\n",
    "    all_y = np.vstack((flat_train_y, flat_test_y))\n",
    "    \n",
    "    # scale \n",
    "    ss = StandardScaler()\n",
    "    all_x = ss.fit_transform(all_x)\n",
    "    \n",
    "    # create X-Validation Split\n",
    "    xvalid_train_X = []\n",
    "    xvalid_train_Y = []\n",
    "    xvalid_test_X = []\n",
    "    xvalid_test_Y = []\n",
    "    width = (int)(all_x.shape[0] / folds)\n",
    "    for i in range(folds):        \n",
    "        xvalid_train_X.append(np.vstack((all_x[:i*width],all_x[(i+1)*width:])))\n",
    "        xvalid_train_Y.append(np.vstack((all_y[:i*width],all_y[(i+1)*width:])))\n",
    "        xvalid_test_X.append(all_x[i*width:(i+1)*width])\n",
    "        xvalid_test_Y.append(all_y[i*width:(i+1)*width])\n",
    "    xvalid_train_X = np.array(xvalid_train_X)\n",
    "    xvalid_train_Y = np.array(xvalid_train_Y)\n",
    "    xvalid_test_X = np.array(xvalid_test_X)\n",
    "    xvalid_test_Y = np.array(xvalid_test_Y)\n",
    "    print(xvalid_train_X.shape)\n",
    "    print(xvalid_train_Y.shape)\n",
    "    print(xvalid_test_X.shape)\n",
    "    print(xvalid_test_Y.shape)\n",
    "    return xvalid_train_X, xvalid_train_Y, xvalid_test_X, xvalid_test_Y\n",
    "\n",
    "xvalid_train_X, xvalid_train_Y, xvalid_test_X, xvalid_test_Y = get_x_valid_data(train_images, train_labels, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 5 different SVD:\n",
    "- **Make sure you change the 'n_jobs' parameter to reflect the \\# of cores in your system**\n",
    "- Outputs each splits AUC value and the Accuracy rate. Also, calculates the average of the values:\n",
    "- You can also turn off 'verbose' parameter to get rid of the unnecessary outputs. You just won't be able to see the progress until the entire process is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1\n",
      "\n",
      "\n",
      "Norm: 383.75, NNZs: 3072, Bias: -3171.468027, T: 48000, Avg. loss: 333.001978\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 419.19, NNZs: 3072, Bias: -2547.048428, T: 48000, Avg. loss: 351.570570\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 497.46, NNZs: 3072, Bias: -3139.069493, T: 48000, Avg. loss: 313.716528\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 377.13, NNZs: 3072, Bias: -2674.653737, T: 48000, Avg. loss: 371.091453\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 343.83, NNZs: 3072, Bias: -2863.003949, T: 96000, Avg. loss: 149.495107\n",
      "Total training time: 1.13 seconds.\n",
      "Convergence after 2 epochs took 1.13 seconds\n",
      "-- Epoch 1\n",
      "Norm: 374.38, NNZs: 3072, Bias: -2102.995665, T: 96000, Avg. loss: 159.298309\n",
      "Total training time: 1.15 seconds.\n",
      "Convergence after 2 epochs took 1.15 seconds\n",
      "-- Epoch 1\n",
      "Norm: 439.47, NNZs: 3072, Bias: -2827.333447, T: 96000, Avg. loss: 142.259803\n",
      "Total training time: 1.16 seconds.\n",
      "Convergence after 2 epochs took 1.16 seconds\n",
      "-- Epoch 1\n",
      "Norm: 323.07, NNZs: 3072, Bias: -2255.942310, T: 96000, Avg. loss: 168.929679\n",
      "Total training time: 1.19 seconds.\n",
      "Convergence after 2 epochs took 1.19 seconds\n",
      "-- Epoch 1\n",
      "Norm: 354.61, NNZs: 3072, Bias: -2750.100218, T: 48000, Avg. loss: 338.310594\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 429.73, NNZs: 3072, Bias: -2892.035510, T: 48000, Avg. loss: 297.590682\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 364.99, NNZs: 3072, Bias: -2813.213765, T: 48000, Avg. loss: 354.507437\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 495.00, NNZs: 3072, Bias: -2814.355842, T: 48000, Avg. loss: 335.400732\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 294.74, NNZs: 3072, Bias: -2355.596217, T: 96000, Avg. loss: 159.490921\n",
      "Total training time: 0.93 seconds.\n",
      "Convergence after 2 epochs took 0.93 seconds\n",
      "-- Epoch 1\n",
      "Norm: 360.01, NNZs: 3072, Bias: -2586.827772, T: 96000, Avg. loss: 132.840710\n",
      "Total training time: 0.91 seconds.\n",
      "Convergence after 2 epochs took 0.91 seconds\n",
      "-- Epoch 1\n",
      "Norm: 319.56, NNZs: 3072, Bias: -2419.019224, T: 96000, Avg. loss: 166.780362\n",
      "Total training time: 0.93 seconds.\n",
      "Convergence after 2 epochs took 0.93 seconds\n",
      "Norm: 441.25, NNZs: 3072, Bias: -2418.696636, T: 96000, Avg. loss: 154.069932\n",
      "Total training time: 0.92 seconds.\n",
      "Convergence after 2 epochs took 0.92 seconds\n",
      "Norm: 376.22, NNZs: 3072, Bias: -3263.020008, T: 48000, Avg. loss: 303.249207\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 419.84, NNZs: 3072, Bias: -3192.180134, T: 48000, Avg. loss: 309.936688\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 332.60, NNZs: 3072, Bias: -2995.879104, T: 96000, Avg. loss: 135.142839\n",
      "Total training time: 0.58 seconds.\n",
      "Convergence after 2 epochs took 0.58 seconds\n",
      "Norm: 369.82, NNZs: 3072, Bias: -2898.516960, T: 96000, Avg. loss: 142.023122\n",
      "Total training time: 0.58 seconds.\n",
      "Convergence after 2 epochs took 0.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 387.57, NNZs: 3072, Bias: -3088.621763, T: 48000, Avg. loss: 319.202634\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 398.07, NNZs: 3072, Bias: -2777.442423, T: 48000, Avg. loss: 379.888517\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 497.52, NNZs: 3072, Bias: -3099.654169, T: 48000, Avg. loss: 310.452386\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 418.84, NNZs: 3072, Bias: -2600.972288, T: 48000, Avg. loss: 359.392351\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 347.45, NNZs: 3072, Bias: -2783.551495, T: 96000, Avg. loss: 144.742079\n",
      "Total training time: 0.67 seconds.\n",
      "Convergence after 2 epochs took 0.67 seconds\n",
      "-- Epoch 1\n",
      "Norm: 435.61, NNZs: 3072, Bias: -2789.593413, T: 96000, Avg. loss: 139.644079\n",
      "Total training time: 0.70 seconds.\n",
      "Convergence after 2 epochs took 0.70 seconds\n",
      "-- Epoch 1\n",
      "Norm: 337.45, NNZs: 3072, Bias: -2352.759343, T: 96000, Avg. loss: 176.607578\n",
      "Total training time: 0.70 seconds.\n",
      "Convergence after 2 epochs took 0.70 seconds\n",
      "-- Epoch 1\n",
      "Norm: 370.52, NNZs: 3072, Bias: -2160.095180, T: 96000, Avg. loss: 162.980304\n",
      "Total training time: 0.70 seconds.\n",
      "Convergence after 2 epochs took 0.70 seconds\n",
      "-- Epoch 1\n",
      "Norm: 362.65, NNZs: 3072, Bias: -2750.416645, T: 48000, Avg. loss: 338.734600\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 374.81, NNZs: 3072, Bias: -2782.348235, T: 48000, Avg. loss: 354.260081\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 505.83, NNZs: 3072, Bias: -2822.186545, T: 48000, Avg. loss: 332.148420\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 432.09, NNZs: 3072, Bias: -3050.646110, T: 48000, Avg. loss: 309.983128\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 304.28, NNZs: 3072, Bias: -2354.283426, T: 96000, Avg. loss: 159.747987\n",
      "Total training time: 0.68 seconds.\n",
      "Convergence after 2 epochs took 0.68 seconds\n",
      "-- Epoch 1\n",
      "Norm: 449.96, NNZs: 3072, Bias: -2429.925883, T: 96000, Avg. loss: 152.891802\n",
      "Total training time: 0.64 seconds.\n",
      "Convergence after 2 epochs took 0.64 seconds\n",
      "-- Epoch 1\n",
      "Norm: 327.77, NNZs: 3072, Bias: -2393.519374, T: 96000, Avg. loss: 163.207656\n",
      "Total training time: 0.66 seconds.\n",
      "Convergence after 2 epochs took 0.66 seconds\n",
      "Norm: 377.72, NNZs: 3072, Bias: -2742.077356, T: 96000, Avg. loss: 139.840855\n",
      "Total training time: 0.66 seconds.\n",
      "Convergence after 2 epochs took 0.66 seconds\n",
      "Norm: 379.45, NNZs: 3072, Bias: -3306.188782, T: 48000, Avg. loss: 312.986317\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 422.65, NNZs: 3072, Bias: -3114.101966, T: 48000, Avg. loss: 310.221801\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 337.86, NNZs: 3072, Bias: -3031.380428, T: 96000, Avg. loss: 139.368349\n",
      "Total training time: 0.54 seconds.\n",
      "Convergence after 2 epochs took 0.54 seconds\n",
      "Norm: 373.20, NNZs: 3072, Bias: -2826.540964, T: 96000, Avg. loss: 136.611659\n",
      "Total training time: 0.53 seconds.\n",
      "Convergence after 2 epochs took 0.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 406.31, NNZs: 3072, Bias: -3215.192104, T: 48000, Avg. loss: 340.401014\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 496.57, NNZs: 3072, Bias: -3143.453454, T: 48000, Avg. loss: 324.062793\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 426.13, NNZs: 3072, Bias: -2649.748024, T: 48000, Avg. loss: 363.578375\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 389.88, NNZs: 3072, Bias: -2692.556255, T: 48000, Avg. loss: 374.398799\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 354.06, NNZs: 3072, Bias: -2914.901807, T: 96000, Avg. loss: 149.642844\n",
      "Total training time: 0.74 seconds.\n",
      "Convergence after 2 epochs took 0.74 seconds\n",
      "-- Epoch 1\n",
      "Norm: 434.91, NNZs: 3072, Bias: -2829.747063, T: 96000, Avg. loss: 142.613165\n",
      "Total training time: 0.83 seconds.\n",
      "Convergence after 2 epochs took 0.83 seconds\n",
      "-- Epoch 1\n",
      "Norm: 377.02, NNZs: 3072, Bias: -2210.153101, T: 96000, Avg. loss: 165.265750\n",
      "Total training time: 0.75 seconds.\n",
      "Convergence after 2 epochs took 0.75 seconds\n",
      "-- Epoch 1\n",
      "Norm: 339.07, NNZs: 3072, Bias: -2269.672257, T: 96000, Avg. loss: 171.161866\n",
      "Total training time: 0.78 seconds.\n",
      "Convergence after 2 epochs took 0.78 seconds\n",
      "-- Epoch 1\n",
      "Norm: 351.71, NNZs: 3072, Bias: -2773.123964, T: 48000, Avg. loss: 341.163411\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 355.18, NNZs: 3072, Bias: -2947.781104, T: 48000, Avg. loss: 362.232393\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 438.35, NNZs: 3072, Bias: -3079.337840, T: 48000, Avg. loss: 314.975326\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 487.42, NNZs: 3072, Bias: -2831.578200, T: 48000, Avg. loss: 332.062929\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 317.02, NNZs: 3072, Bias: -2371.482371, T: 96000, Avg. loss: 161.771167\n",
      "Total training time: 0.66 seconds.\n",
      "Convergence after 2 epochs took 0.66 seconds\n",
      "-- Epoch 1\n",
      "Norm: 333.85, NNZs: 3072, Bias: -2558.144947, T: 96000, Avg. loss: 171.593852\n",
      "Total training time: 0.64 seconds.\n",
      "Convergence after 2 epochs took 0.64 seconds\n",
      "-- Epoch 1\n",
      "Norm: 389.56, NNZs: 3072, Bias: -2767.152203, T: 96000, Avg. loss: 141.169168\n",
      "Total training time: 0.64 seconds.\n",
      "Convergence after 2 epochs took 0.64 seconds\n",
      "Norm: 449.10, NNZs: 3072, Bias: -2437.713038, T: 96000, Avg. loss: 154.515069\n",
      "Total training time: 0.64 seconds.\n",
      "Convergence after 2 epochs took 0.64 seconds\n",
      "Norm: 375.51, NNZs: 3072, Bias: -3257.415760, T: 48000, Avg. loss: 303.492202\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 413.29, NNZs: 3072, Bias: -3127.844169, T: 48000, Avg. loss: 313.140027\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 334.86, NNZs: 3072, Bias: -2986.775605, T: 96000, Avg. loss: 135.706344\n",
      "Total training time: 0.54 seconds.\n",
      "Convergence after 2 epochs took 0.54 seconds\n",
      "Norm: 373.84, NNZs: 3072, Bias: -2835.177546, T: 96000, Avg. loss: 139.436492\n",
      "Total training time: 0.51 seconds.\n",
      "Convergence after 2 epochs took 0.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 499.96, NNZs: 3072, Bias: -3142.138290, T: 48000, Avg. loss: 317.970326\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 381.17, NNZs: 3072, Bias: -2780.138881, T: 48000, Avg. loss: 380.206000\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 380.25, NNZs: 3072, Bias: -3067.352730, T: 48000, Avg. loss: 327.534807\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 411.68, NNZs: 3072, Bias: -2592.498245, T: 48000, Avg. loss: 361.913309\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 432.73, NNZs: 3072, Bias: -2830.141345, T: 96000, Avg. loss: 143.057781\n",
      "Total training time: 0.83 seconds.\n",
      "Convergence after 2 epochs took 0.83 seconds\n",
      "-- Epoch 1\n",
      "Norm: 343.64, NNZs: 3072, Bias: -2356.445698, T: 96000, Avg. loss: 175.906747\n",
      "Total training time: 0.87 seconds.\n",
      "Convergence after 2 epochs took 0.87 seconds\n",
      "-- Epoch 1\n",
      "Norm: 343.24, NNZs: 3072, Bias: -2766.371847, T: 96000, Avg. loss: 143.281113\n",
      "Total training time: 0.87 seconds.\n",
      "Convergence after 2 epochs took 0.87 seconds\n",
      "-- Epoch 1\n",
      "Norm: 364.06, NNZs: 3072, Bias: -2149.756095, T: 96000, Avg. loss: 163.956966\n",
      "Total training time: 0.92 seconds.\n",
      "Convergence after 2 epochs took 0.92 seconds\n",
      "-- Epoch 1\n",
      "Norm: 364.68, NNZs: 3072, Bias: -2912.085655, T: 48000, Avg. loss: 347.486795\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 414.02, NNZs: 3072, Bias: -2978.816919, T: 48000, Avg. loss: 307.415184\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 499.59, NNZs: 3072, Bias: -2811.597403, T: 48000, Avg. loss: 336.042660\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 361.98, NNZs: 3072, Bias: -2866.420743, T: 48000, Avg. loss: 356.135693\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 323.53, NNZs: 3072, Bias: -2515.265159, T: 96000, Avg. loss: 166.839617\n",
      "Total training time: 1.27 seconds.\n",
      "Convergence after 2 epochs took 1.27 seconds\n",
      "-- Epoch 1\n",
      "Norm: 445.40, NNZs: 3072, Bias: -2418.681126, T: 96000, Avg. loss: 153.967003\n",
      "Total training time: 1.38 seconds.\n",
      "Convergence after 2 epochs took 1.38 seconds\n",
      "-- Epoch 1\n",
      "Norm: 386.37, NNZs: 3072, Bias: -2663.345165, T: 96000, Avg. loss: 137.386024\n",
      "Total training time: 1.45 seconds.\n",
      "Convergence after 2 epochs took 1.45 seconds\n",
      "Norm: 330.16, NNZs: 3072, Bias: -2478.803013, T: 96000, Avg. loss: 167.490003\n",
      "Total training time: 1.59 seconds.\n",
      "Convergence after 2 epochs took 1.59 seconds\n",
      "Norm: 373.41, NNZs: 3072, Bias: -3211.544164, T: 48000, Avg. loss: 306.492406\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 399.47, NNZs: 3072, Bias: -3151.964810, T: 48000, Avg. loss: 322.301381\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 341.25, NNZs: 3072, Bias: -2937.945292, T: 96000, Avg. loss: 135.666448\n",
      "Total training time: 0.86 seconds.\n",
      "Convergence after 2 epochs took 0.86 seconds\n",
      "Norm: 378.55, NNZs: 3072, Bias: -2857.398958, T: 96000, Avg. loss: 140.366314\n",
      "Total training time: 0.72 seconds.\n",
      "Convergence after 2 epochs took 0.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 384.61, NNZs: 3072, Bias: -3059.017051, T: 48000, Avg. loss: 326.786687\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 427.96, NNZs: 3072, Bias: -2625.631646, T: 48000, Avg. loss: 366.191125\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 376.07, NNZs: 3072, Bias: -2742.414743, T: 48000, Avg. loss: 370.284388\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 500.65, NNZs: 3072, Bias: -3134.364848, T: 48000, Avg. loss: 320.834704\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 349.14, NNZs: 3072, Bias: -2750.904846, T: 96000, Avg. loss: 145.227474\n",
      "Total training time: 0.83 seconds.\n",
      "Convergence after 2 epochs took 0.83 seconds\n",
      "-- Epoch 1\n",
      "Norm: 381.23, NNZs: 3072, Bias: -2187.088570, T: 96000, Avg. loss: 163.753601\n",
      "Total training time: 0.84 seconds.\n",
      "Convergence after 2 epochs took 0.84 seconds\n",
      "-- Epoch 1\n",
      "Norm: 335.04, NNZs: 3072, Bias: -2321.415372, T: 96000, Avg. loss: 172.759128\n",
      "Total training time: 0.86 seconds.\n",
      "Convergence after 2 epochs took 0.86 seconds\n",
      "-- Epoch 1\n",
      "Norm: 441.15, NNZs: 3072, Bias: -2824.852905, T: 96000, Avg. loss: 140.862143\n",
      "Total training time: 0.86 seconds.\n",
      "Convergence after 2 epochs took 0.86 seconds\n",
      "-- Epoch 1\n",
      "Norm: 370.83, NNZs: 3072, Bias: -2842.683699, T: 48000, Avg. loss: 343.387922\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 411.23, NNZs: 3072, Bias: -2911.407381, T: 48000, Avg. loss: 301.211198\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 353.50, NNZs: 3072, Bias: -2822.740619, T: 48000, Avg. loss: 353.427733\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 504.38, NNZs: 3072, Bias: -2856.312323, T: 48000, Avg. loss: 334.442326\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 322.22, NNZs: 3072, Bias: -2437.378353, T: 96000, Avg. loss: 166.697579\n",
      "Total training time: 0.79 seconds.\n",
      "Convergence after 2 epochs took 0.79 seconds\n",
      "-- Epoch 1\n",
      "Norm: 384.04, NNZs: 3072, Bias: -2598.602400, T: 96000, Avg. loss: 133.980191\n",
      "Total training time: 0.77 seconds.\n",
      "Convergence after 2 epochs took 0.77 seconds\n",
      "-- Epoch 1\n",
      "Norm: 333.94, NNZs: 3072, Bias: -2434.097240, T: 96000, Avg. loss: 164.812751\n",
      "Total training time: 0.80 seconds.\n",
      "Convergence after 2 epochs took 0.80 seconds\n",
      "Norm: 450.17, NNZs: 3072, Bias: -2465.350639, T: 96000, Avg. loss: 155.921060\n",
      "Total training time: 0.79 seconds.\n",
      "Convergence after 2 epochs took 0.79 seconds\n",
      "Norm: 422.08, NNZs: 3072, Bias: -3127.927651, T: 48000, Avg. loss: 316.580407Norm: 375.95, NNZs: 3072, Bias: -3300.790888, T: 48000, Avg. loss: 300.331093\n",
      "Total training time: 0.36 seconds.\n",
      "\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 374.43, NNZs: 3072, Bias: -2839.588687, T: 96000, Avg. loss: 138.469526\n",
      "Total training time: 0.64 seconds.\n",
      "Convergence after 2 epochs took 0.64 seconds\n",
      "Norm: 343.28, NNZs: 3072, Bias: -3026.903938, T: 96000, Avg. loss: 138.076888\n",
      "Total training time: 0.66 seconds.\n",
      "Convergence after 2 epochs took 0.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "0 - AUC: 0.31\tACC: 0.39\n",
      "1 - AUC: 0.42\tACC: 0.37\n",
      "2 - AUC: 0.41\tACC: 0.38\n",
      "3 - AUC: 0.47\tACC: 0.39\n",
      "4 - AUC: 0.47\tACC: 0.39\n",
      "================================\n",
      "    AUC: 0.42\tACC: 0.38\n"
     ]
    }
   ],
   "source": [
    "folds = 5\n",
    "auc_list = []\n",
    "acc_list = []\n",
    "# fit models and get results\n",
    "for i in range(folds):\n",
    "    sgdClassifierAll = linear_model.SGDClassifier(loss=\"hinge\", penalty='l2',\n",
    "                    alpha=0.0001, l1_ratio=0.15, fit_intercept=True,\n",
    "                    max_iter=1000, tol=1e-4, shuffle=True, verbose=1,\n",
    "                    epsilon=0.1, n_jobs=4, random_state=1010, learning_rate='optimal',\n",
    "                    eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1,\n",
    "                    n_iter_no_change=10, class_weight=None, warm_start=False,\n",
    "                    average=False)\n",
    "    sgdClassifierAll.fit(xvalid_train_X[i], xvalid_train_Y[i])\n",
    "    predictions = sgdClassifierAll.predict(xvalid_test_X[i])\n",
    "    \n",
    "    # calculate validation metrics\n",
    "    a_sum = 0\n",
    "    for j in range(10): # every class\n",
    "        fpr, tpr, thresholds = roc_curve(xvalid_test_Y[i], predictions, pos_label=i)\n",
    "        a_sum += auc(fpr, tpr)\n",
    "    a_average = a_sum / 10\n",
    "    b = accuracy_score(xvalid_test_Y[i], predictions)\n",
    "    auc_list.append(a_average)\n",
    "    acc_list.append(b)\n",
    "    \n",
    "\n",
    "# print results\n",
    "for i in range(folds):\n",
    "    print(\"{:d} - AUC: {:.2f}\\tACC: {:.2f}\".format(i, auc_list[i], acc_list[i]))\n",
    "print(\"================================\")\n",
    "mean_auc = sum(auc_list) / folds\n",
    "mean_acc = sum(acc_list) / folds\n",
    "print(\"    AUC: {:.2f}\\tACC: {:.2f}\".format(mean_auc, mean_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
